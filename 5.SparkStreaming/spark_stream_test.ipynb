{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proper-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pyspark.sql.functions import window\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "structured-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wound-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "# from pyspark.streaming import StreamingContext\n",
    "# sc = SparkContext(\"local\", \"WordCount\")\n",
    "# ssc = StreamingContext(sc, 10) // Stream data every 10 sec\n",
    "# lines = ssc.textFileStream(\"/home/karthiamrita/textdata\")\n",
    "# words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "# # Count each word in each batch\n",
    "# pairs = words.map(lambda word: (word, 1))\n",
    "# wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "# # Print the first 10 \n",
    "# elements of each RDD generated in this DStream to the console\n",
    "# wordCounts.pprint()\n",
    "# ssc.start() # Start the computation\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "foreign-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "import pyspark.sql.types as tp\n",
    "# Create a spark Session\n",
    "spark = SparkSession.builder.appName(\"StructuredStreaming\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efficient-audience",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_schema = tp.StructType([\n",
    "  tp.StructField(name= 'mobileno',dataType= tp.StringType(),  nullable= True),\n",
    "  tp.StructField(name= 'value',dataType= tp.StringType(),   nullable= True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coastal-hearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create the schema for dataframe\n",
    "#userSchema = StructType().add(\"mobileno\", StringType()).add(\"dateandtime\", float).add(\"acc\",float).add(\"gyro\",float).add(\"weight\",Integer).add(\"height\",Integer).add(\"Gender\",string).add(\"age\",Integer)\n",
    "# read the data from folder - contains all the CSV file\n",
    "csvDF = spark.readStream.format(\"csv\").option(\"header\", True).schema(user_schema).load(\"/home/jovyan/work/mldb_proj//data\")\n",
    "csvDF.isStreaming\n",
    "#csvDF.show()\n",
    "#query = csvDF.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "#query.awaitTermination()\n",
    "#query=csvDF.writeStream.format(\"csv\").trigger(processingTime=\"10 seconds\").option(\"checkpointLocation\", \"/home/jovyan/work/mldb_proj\").option(\"path\",\"/home/jovyan/work/mldb_proj//file\").outputMode(\"append\").start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reliable-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvDF.createOrReplaceTempView(\"mobile_position\");\n",
    "#orders_find_text = spark.sql(\"SELECT * FROM orders_find\")\n",
    "sensor_write_stream = csvDF \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='5 seconds') \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"truncate\", \"false\") \\\n",
    "        .format('memory') \\\n",
    "        .queryName(\"sensorTable\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grand-premium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      40|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-441197524a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#    # acc1, gyro1  = get_data_mobile1(df.tail(2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     print(acc1, gyro1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    count=spark.sql(\"SELECT count(*) FROM sensorTable\" )\n",
    "    count.show()\n",
    "   # df = spark.sql(\"SELECT * FROM sensorTable\" )\n",
    "#    # acc1, gyro1  = get_data_mobile1(df.tail(2))\n",
    "#     print(acc1, gyro1)\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "danish-scroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[mobileno: string, value: string]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "chicken-french",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-0e566b70f572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "df.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cordless-chinese",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.91517029606653 0.0\n"
     ]
    }
   ],
   "source": [
    "acc1, gyro1  = get_data_mobile1(df.tail(2))\n",
    "print(acc1, gyro1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "classical-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_mobile1(data):\n",
    "    \n",
    "    mobile1_values = data[0][1]\n",
    "    x = list(map(float, mobile1_values.split('_')))\n",
    "    mobile = \"mobile1\"\n",
    "    sqrt_acceler  = 0\n",
    "    sqrt_gyro = 0\n",
    "    i = 0\n",
    "    for item in x:  \n",
    "        i = i+1\n",
    "        if item == 3.0:\n",
    "            acceleros=[x[i],x[i+1],x[i+2]]\n",
    "            sqrt_acceler = np.sqrt(sum(j*j for j in acceleros))\n",
    "#             print(sqrt_acceler)\n",
    "\n",
    "        if item == 4.0:\n",
    "            gyros=[x[i],x[i+1],x[i+2]]\n",
    "            sqrt_gyro = np.sqrt(sum(j*j for j in gyros))\n",
    "#             print(sqrt_gyro)\n",
    "            \n",
    "    return float(sqrt_acceler), float(sqrt_gyro)\n",
    "   # return mobile, mobile1_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "productive-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_mobile2(data):\n",
    "    \n",
    "    mobile1_values = data[1][1]\n",
    "    x = list(map(float, mobile1_values.split('_')))\n",
    "    mobile = \"mobile1\"\n",
    "    sqrt_acceler  = 0\n",
    "    sqrt_gyro = 0\n",
    "    i = 0\n",
    "    for item in x:  \n",
    "        i = i+1\n",
    "        if item == 3.0:\n",
    "            acceleros=[x[i],x[i+1],x[i+2]]\n",
    "            sqrt_acceler = np.sqrt(sum(j*j for j in acceleros))\n",
    "#             print(sqrt_acceler)\n",
    "\n",
    "        if item == 4.0:\n",
    "            gyros=[x[i],x[i+1],x[i+2]]\n",
    "            sqrt_gyro = np.sqrt(sum(j*j for j in gyros))\n",
    "#             print(sqrt_gyro)\n",
    "            \n",
    "    return mobile, float(sqrt_acceler), float(sqrt_gyro)\n",
    "   # return mobile, mobile1_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "elder-summit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----+\n",
      "|mobileno|     acc|gyro|\n",
      "+--------+--------+----+\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "| mobile1|9.915171| 0.0|\n",
      "+--------+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = spark.sql(\"SELECT * FROM sensorTable \\\n",
    "#                 CLUSTER BY mobileno\" )\n",
    "# #a=df.groupby(\"mobileno\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "opened-indian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(mobileno='mobile1', acc=9.915170669555664, gyro=0.0),\n",
       " Row(mobileno='mobile2', acc=9.841073036193848, gyro=0.0)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "amazing-egypt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.948163032531738, 9.841073036193848]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = df.count()\n",
    "df.rdd.zipWithIndex()\\\n",
    "  .filter(lambda x : x[1] == 0 or x[1] == size-1)\\\n",
    "  .map(lambda x : x[0].acc)\\\n",
    "  .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "swedish-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # query=csvDF.writeStream.format(\"csv\").option(\"checkpointLocation\", \"/home/jovyan/work/mldb_proj\").option(\"path\",\"/home/jovyan/work/mldb_proj//file\").outputMode(\"append\").toTable(\"myTable\").start()\n",
    "# spark.read.table(\"myTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "former-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_row(row):\n",
    "    \n",
    "#     # Write row to storage\n",
    "#     print(row)\n",
    "# csvDF = spark.readStream.format(\"csv\").option(\"header\", True).schema(user_schema).load(\"/home/jovyan/work/mldb_proj//data\")\n",
    "# query = csvDF.writeStream.foreach(process_row).start()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "secure-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.readStream \\\n",
    "#     .format(\"rate\") \\\n",
    "#     .option(\"rowsPerSecond\", 10) \\\n",
    "#     .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write the streaming DataFrame to a table\n",
    "# df.writeStream \\\n",
    "#     .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "#     .toTable(\"myTable\")\n",
    "# # Check the table result\n",
    "# spark.read.table(\"myTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "affected-syndicate",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\nrate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-dace1291db51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();\nrate"
     ]
    }
   ],
   "source": [
    "# df.show()\n",
    "# query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "upset-consumer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mobileno: string (nullable = true)\n",
      " |-- dateandtime: string (nullable = true)\n",
      " |-- acc: float (nullable = true)\n",
      " |-- gyro: float (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "rapid-chamber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'mobileno'>\n",
      "Column<'dateandtime'>\n",
      "Column<'acc'>\n",
      "Column<'gyro'>\n",
      "Column<'weight'>\n",
      "Column<'height'>\n",
      "Column<'gender'>\n",
      "Column<'age'>\n"
     ]
    }
   ],
   "source": [
    "for x in csvDF:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "frozen-hollywood",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`acc1`' given input columns: [acc, age, dateandtime, gender, gyro, height, mobileno, weight];\n'Aggregate [sum('acc1) AS sum(acc1)#195]\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4c545b73,csv,List(),Some(StructType(StructField(mobileno,StringType,true), StructField(dateandtime,StringType,true), StructField(acc,FloatType,true), StructField(gyro,FloatType,true), StructField(weight,IntegerType,true), StructField(height,IntegerType,true), StructField(gender,StringType,true), StructField(age,IntegerType,true))),List(),None,Map(sep -> ,, header -> true, path -> /home/jovyan/work/mldb_proj//data),None), FileSource[/home/jovyan/work/mldb_proj//data], [mobileno#170, dateandtime#171, acc#172, gyro#173, weight#174, height#175, gender#176, age#177]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2f97641ddd73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcsvDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sep\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/jovyan/work/mldb_proj//data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#csvDF_time = csvDF.withColumn(\"time_st\",f.current_timestamp())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwindowed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsvDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mwindowed_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"counts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m   1814\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m         \"\"\"\n\u001b[0;32m-> 1816\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# Columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             jdf = self._jgd.agg(exprs[0]._jc,\n\u001b[0m\u001b[1;32m    119\u001b[0m                                 _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`acc1`' given input columns: [acc, age, dateandtime, gender, gyro, height, mobileno, weight];\n'Aggregate [sum('acc1) AS sum(acc1)#195]\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@4c545b73,csv,List(),Some(StructType(StructField(mobileno,StringType,true), StructField(dateandtime,StringType,true), StructField(acc,FloatType,true), StructField(gyro,FloatType,true), StructField(weight,IntegerType,true), StructField(height,IntegerType,true), StructField(gender,StringType,true), StructField(age,IntegerType,true))),List(),None,Map(sep -> ,, header -> true, path -> /home/jovyan/work/mldb_proj//data),None), FileSource[/home/jovyan/work/mldb_proj//data], [mobileno#170, dateandtime#171, acc#172, gyro#173, weight#174, height#175, gender#176, age#177]\n"
     ]
    }
   ],
   "source": [
    "#csvDF = spark.readStream.format(\"csv\").option(\"header\", True).schema(user_schema).csv(\"/home/jovyan/work/mldb_proj//data\")\n",
    "csvDF = spark.readStream.option(\"sep\", \",\").option(\"header\" , True).schema(user_schema).csv(\"/home/jovyan/work/mldb_proj//data\")\n",
    "#csvDF_time = csvDF.withColumn(\"time_st\",f.current_timestamp())\n",
    "windowed_df = csvDF.agg(f.sum(\"acc1\"))\n",
    "query = ( windowed_df.writeStream.outputMode(\"complete\").format(\"console\").queryName(\"counts\").start() )\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-duplicate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "improved-fluid",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e73e23718a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'query' is not defined"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "billion-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(csvDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fossil-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-pursuit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
