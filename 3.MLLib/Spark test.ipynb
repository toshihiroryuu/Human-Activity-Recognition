{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL : https://github.com/nxs5899/end-to-end-Machine-Learning-model-with-MLlib-in-pySpark/blob/master/MLlib_pySpark.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL : https://towardsdatascience.com/build-an-end-to-end-machine-learning-model-with-mllib-in-pyspark-4917bdf289c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
      "Collecting py4j==0.10.9\n",
      "  Using cached py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=fc464862f4474fbf64636821acaedcd2c47784a18bfd2971ee0e6cc58d7048f3\n",
      "  Stored in directory: c:\\users\\jiss\\appdata\\local\\pip\\cache\\wheels\\df\\88\\9e\\58ef1f74892fef590330ca0830b5b6d995ba29b44f977b3926\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the 'D:\\Softwares\\AnacondaInstall\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark[sql] in d:\\softwares\\anacondainstall\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: py4j==0.10.9 in d:\\softwares\\anacondainstall\\lib\\site-packages (from pyspark[sql]) (0.10.9)\n",
      "Requirement already satisfied: pandas>=0.23.2 in d:\\softwares\\anacondainstall\\lib\\site-packages (from pyspark[sql]) (1.2.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the 'D:\\Softwares\\AnacondaInstall\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\softwares\\anacondainstall\\lib\\site-packages (from pandas>=0.23.2->pyspark[sql]) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in d:\\softwares\\anacondainstall\\lib\\site-packages (from pandas>=0.23.2->pyspark[sql]) (1.20.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\softwares\\anacondainstall\\lib\\site-packages (from pandas>=0.23.2->pyspark[sql]) (2021.1)\n",
      "Collecting pyarrow>=1.0.0\n",
      "  Downloading pyarrow-4.0.1-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\softwares\\anacondainstall\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.23.2->pyspark[sql]) (1.15.0)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-4.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark[sql]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\SparkMLBD\\\\Spark'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\SparkMLBD\\\\Spark'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparkConf().setAppName('SparkApp').setMaster('local')\n",
    "sc= SparkContext(conf=config)\n",
    "spark = SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda create -n pyspark_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !PYSPARK_HADOOP_VERSION=3.2 pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "       .master(\"local\") \\\n",
    "       .appName(\"Athulspark\") \\\n",
    "       .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "       .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- rotationRate: double (nullable = true)\n",
      " |-- userAcceleration: double (nullable = true)\n",
      " |-- act: double (nullable = true)\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- gender: double (nullable = true)\n",
      " |-- trial: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = spark.read.csv('athul_test.csv', header=True, inferSchema=True)\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_lbl = [\"Sat\", \"Stand-Up\", \"Downstairs\", \"Upstairs\", \"Walking\", \"Jogging\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>234657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>224816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>158645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>58204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>50246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>104327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label   count\n",
       "0    0.0  234657\n",
       "1    1.0  224816\n",
       "2    4.0  158645\n",
       "3    3.0   58204\n",
       "4    2.0   50246\n",
       "5    5.0  104327"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_col = ['_c0', 'id', 'trial']\n",
    "\n",
    "new_df = new_df.select([column for column in new_df.columns if column not in drop_col])\n",
    "new_df = new_df.withColumnRenamed('act', 'label')\n",
    "new_df.groupby('label').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "df_pd = new_df.toPandas()\n",
    "print(len(df_pd))\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.countplot(x='label', data=df_pd, order=df_pd['label'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotationRate</th>\n",
       "      <th>userAcceleration</th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010253</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010920</td>\n",
       "      <td>0.010673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.007010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006555</td>\n",
       "      <td>0.014892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.013001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008544</td>\n",
       "      <td>0.008358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010706</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010136</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.010180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rotationRate  userAcceleration  label  weight  height   age  gender\n",
       "0      0.010253          0.006959    0.0   102.0   188.0  46.0     1.0\n",
       "1      0.010920          0.010673    0.0   102.0   188.0  46.0     1.0\n",
       "2      0.008377          0.007010    0.0   102.0   188.0  46.0     1.0\n",
       "3      0.006555          0.014892    0.0   102.0   188.0  46.0     1.0\n",
       "4      0.007724          0.013001    0.0   102.0   188.0  46.0     1.0\n",
       "5      0.008544          0.008358    0.0   102.0   188.0  46.0     1.0\n",
       "6      0.010706          0.007313    0.0   102.0   188.0  46.0     1.0\n",
       "7      0.006017          0.011407    0.0   102.0   188.0  46.0     1.0\n",
       "8      0.010136          0.008716    0.0   102.0   188.0  46.0     1.0\n",
       "9      0.011316          0.010180    0.0   102.0   188.0  46.0     1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(new_df.take(10), columns= new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  categorical features\n",
      "6  numerical features\n"
     ]
    }
   ],
   "source": [
    "cat_cols = [item[0] for item in new_df.dtypes if item[1].startswith('string')] \n",
    "print(str(len(cat_cols)) + '  categorical features')\n",
    "\n",
    "num_cols = [item[0] for item in new_df.dtypes if item[1].startswith('int') | item[1].startswith('double')][1:]\n",
    "print(str(len(num_cols)) + '  numerical features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_missing_table(df_pd):\n",
    "    \"\"\"Input pandas dataframe and Return columns with missing value and percentage\"\"\"\n",
    "    mis_val = df_pd.isnull().sum() #count total of null in each columns in dataframe\n",
    "#count percentage of null in each columns\n",
    "    mis_val_percent = 100 * df_pd.isnull().sum() / len(df_pd) \n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1) \n",
    " #join to left (as column) between mis_val and mis_val_percent\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'}) \n",
    "#rename columns in table\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "    mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1) \n",
    "        \n",
    "    print (\"Your selected dataframe has \" + str(df_pd.shape[1]) + \" columns.\\n\"    #.shape[1] : just view total columns in dataframe  \n",
    "    \"There are \" + str(mis_val_table_ren_columns.shape[0]) +              \n",
    "    \" columns that have missing values.\") #.shape[0] : just view total rows in dataframe\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 7 columns.\n",
      "There are 0 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Values, % of Total Values]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missings = info_missing_table(df_pd)\n",
    "missings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missings(spark_df):\n",
    "    null_counts = []        \n",
    "    for col in spark_df.dtypes:    \n",
    "        cname = col[0]     \n",
    "        ctype = col[1]      \n",
    "        nulls = spark_df.where( spark_df[cname].isNull()).count() #check count of null in column name\n",
    "        result = tuple([cname, nulls])  #new tuple, (column name, null count)\n",
    "        null_counts.append(result)      #put the new tuple in our result list\n",
    "    null_counts=[(x,y) for (x,y) in null_counts if y!=0]  #view just columns that have missing values\n",
    "    return null_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_counts = count_missings(new_df)\n",
    "miss_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding the new column weights and fill it with ratios\n",
    "# from pyspark.sql.functions import when\n",
    "\n",
    "# ratio = 0.91\n",
    "# def weight_balance(labels):\n",
    "#     return when(labels == 1, ratio).otherwise(1*(1-ratio))\n",
    "\n",
    "# new_df = new_df.withColumn('weights', weight_balance(col('label')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotationRate</th>\n",
       "      <th>userAcceleration</th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010253</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010920</td>\n",
       "      <td>0.010673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.007010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006555</td>\n",
       "      <td>0.014892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.013001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.008544</td>\n",
       "      <td>0.008358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010706</td>\n",
       "      <td>0.007313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010136</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.011316</td>\n",
       "      <td>0.010180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rotationRate  userAcceleration  label  weight  height   age  gender\n",
       "0      0.010253          0.006959    0.0   102.0   188.0  46.0     1.0\n",
       "1      0.010920          0.010673    0.0   102.0   188.0  46.0     1.0\n",
       "2      0.008377          0.007010    0.0   102.0   188.0  46.0     1.0\n",
       "3      0.006555          0.014892    0.0   102.0   188.0  46.0     1.0\n",
       "4      0.007724          0.013001    0.0   102.0   188.0  46.0     1.0\n",
       "5      0.008544          0.008358    0.0   102.0   188.0  46.0     1.0\n",
       "6      0.010706          0.007313    0.0   102.0   188.0  46.0     1.0\n",
       "7      0.006017          0.011407    0.0   102.0   188.0  46.0     1.0\n",
       "8      0.010136          0.008716    0.0   102.0   188.0  46.0     1.0\n",
       "9      0.011316          0.010180    0.0   102.0   188.0  46.0     1.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(new_df.take(10), columns= new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "stages = []\n",
    "for categoricalCol in cat_cols:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "assemblerInputs = [c + \"classVec\" for c in cat_cols] + num_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "cols = new_df.columns\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(new_df)\n",
    "new_df = pipelineModel.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>rotationRate</th>\n",
       "      <th>userAcceleration</th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.006959199379238966, 0.0, 102.0, 188.0, 46.0...</td>\n",
       "      <td>0.010253</td>\n",
       "      <td>0.006959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.010672920359489243, 0.0, 102.0, 188.0, 46.0...</td>\n",
       "      <td>0.010920</td>\n",
       "      <td>0.010673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0070096587648757905, 0.0, 102.0, 188.0, 46....</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.007010</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.014892331247994722, 0.0, 102.0, 188.0, 46.0...</td>\n",
       "      <td>0.006555</td>\n",
       "      <td>0.014892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.013001225519157802, 0.0, 102.0, 188.0, 46.0...</td>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.013001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features  rotationRate  \\\n",
       "0  [0.006959199379238966, 0.0, 102.0, 188.0, 46.0...      0.010253   \n",
       "1  [0.010672920359489243, 0.0, 102.0, 188.0, 46.0...      0.010920   \n",
       "2  [0.0070096587648757905, 0.0, 102.0, 188.0, 46....      0.008377   \n",
       "3  [0.014892331247994722, 0.0, 102.0, 188.0, 46.0...      0.006555   \n",
       "4  [0.013001225519157802, 0.0, 102.0, 188.0, 46.0...      0.007724   \n",
       "\n",
       "   userAcceleration  label  weight  height   age  gender  \n",
       "0          0.006959    0.0   102.0   188.0  46.0     1.0  \n",
       "1          0.010673    0.0   102.0   188.0  46.0     1.0  \n",
       "2          0.007010    0.0   102.0   188.0  46.0     1.0  \n",
       "3          0.014892    0.0   102.0   188.0  46.0     1.0  \n",
       "4          0.013001    0.0   102.0   188.0  46.0     1.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedCols = ['features']+cols\n",
    "new_df = new_df.select(selectedCols)\n",
    "pd.DataFrame(new_df.take(5), columns=new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665002\n",
      "165893\n"
     ]
    }
   ],
   "source": [
    "# split the data into trainign and testin sets\n",
    "\n",
    "train, test = new_df.randomSplit([0.80, 0.20], seed = 42)\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we check how LogisticRegression perform \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=15)\n",
    "LR_model = LR.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      "DenseMatrix([[-1.26544111e+01, -8.15296408e+00, -2.96777156e-03,\n",
      "               7.76273200e-02,  3.56301098e-02, -3.96039342e-01],\n",
      "             [-1.14428792e+01, -3.58723835e+00,  6.74197593e-03,\n",
      "               5.76398703e-02,  5.66946178e-02,  1.11082274e-01],\n",
      "             [ 1.58199502e+00, -1.29314214e-01, -2.01308657e-03,\n",
      "               3.91864139e-02, -1.78357894e-02, -2.49877423e-01],\n",
      "             [ 7.87508315e-01,  1.02485378e+00,  1.87902048e-03,\n",
      "               2.47650773e-02, -1.50995412e-02, -1.47456518e-01],\n",
      "             [ 6.88825045e+00,  5.24097407e+00,  3.69467437e-03,\n",
      "              -7.40988493e-02, -5.11317288e-02,  1.12107310e+00],\n",
      "             [ 1.48395365e+01,  5.60368879e+00, -7.33481265e-03,\n",
      "              -1.25119832e-01, -8.25766827e-03, -4.38782093e-01]])\n",
      "Intercept: [0.7211368991798303,0.6883807737244825,-0.8132311399024507,-0.6784052184722136,0.23821847008609687,-0.15609978461574509]\n",
      "objectiveHistory:\n",
      "1.6436897894180902\n",
      "1.4401408951850654\n",
      "1.1277437019689573\n",
      "0.9926709185167675\n",
      "0.8855719032094621\n",
      "0.7517901816238962\n",
      "0.6510338283577126\n",
      "0.5602305783454845\n",
      "0.5300973147459217\n",
      "0.5160505755394711\n",
      "0.510974166927072\n",
      "0.506315407521886\n",
      "0.5013024170957386\n",
      "0.4845824928702314\n",
      "0.471754115652064\n",
      "0.46188300048565234\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \\n\" + str(LR_model.coefficientMatrix))\n",
    "print(\"Intercept: \" + str(LR_model.interceptVector))\n",
    "\n",
    "trainingSummary = LR_model.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positive rate by label:\n",
      "label 0: 0.0\n",
      "label 1: 0.00041009028168462614\n",
      "label 2: 0.0002961407260730299\n",
      "label 3: 0.03003030195232536\n",
      "label 4: 0.06536046135483943\n",
      "label 5: 0.03924809598833169\n",
      "True positive rate by label:\n",
      "label 0: 0.9999254105085937\n",
      "label 1: 0.9990486416717201\n",
      "label 2: 0.4845281520633266\n",
      "label 3: 0.7795962199312715\n",
      "label 4: 0.8537778896686282\n",
      "label 5: 0.6755185530754324\n",
      "Precision by label:\n",
      "label 0: 1.0\n",
      "label 1: 0.9988930361404231\n",
      "label 2: 0.9906143777586119\n",
      "label 3: 0.6615272462183343\n",
      "label 4: 0.7553087089971394\n",
      "label 5: 0.712211978660882\n",
      "Recall by label:\n",
      "label 0: 0.9999254105085937\n",
      "label 1: 0.9990486416717201\n",
      "label 2: 0.4845281520633266\n",
      "label 3: 0.7795962199312715\n",
      "label 4: 0.8537778896686282\n",
      "label 5: 0.6755185530754324\n",
      "F-measure by label:\n",
      "label 0: 0.999962703863347\n",
      "label 1: 0.9989708328465651\n",
      "label 2: 0.6507582069655058\n",
      "label 3: 0.715725130631963\n",
      "label 4: 0.8015303541159656\n",
      "label 5: 0.6933801545837963\n"
     ]
    }
   ],
   "source": [
    "#for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8843131298853236\n",
      "FPR: 0.01965822018857779\n",
      "TPR: 0.8843131298853237\n",
      "F-measure: 0.8821630545860956\n",
      "Precision: 0.892485713973054\n",
      "Recall: 0.8843131298853237\n"
     ]
    }
   ],
   "source": [
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = LR_model.transform(test)\n",
    "predictions.select(\"prediction\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8838379033267365"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = [\"rotationRate\", \"userAcceleration\",\"weight\", \"height\", \"age\", \"gender\"]\n",
    "\n",
    "outputCol = \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(0.010253, 0.006959, 102.0, 188.0, 46.0, 1.0)], inputCols)\n",
    "# df = spark.createDataFrame([(3.8651705114155055,1.6289638410434408,70.0,180.0,35.0,1.0)], inputCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_va = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "\n",
    "df = df_va.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = LR_model.transform(df)\n",
    "prediction.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "DT_model = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions.\n",
    "predictions = DT_model.transform(df)\n",
    "predictions.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.3.3; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the 'D:\\Softwares\\AnacondaInstall\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading gradio-2.0.2-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: pillow in d:\\softwares\\anacondainstall\\lib\\site-packages (from gradio) (8.2.0)\n",
      "Requirement already satisfied: requests in d:\\softwares\\anacondainstall\\lib\\site-packages (from gradio) (2.25.1)\n",
      "Requirement already satisfied: Flask>=1.1.1 in d:\\softwares\\anacondainstall\\lib\\site-packages (from gradio) (1.1.2)\n",
      "Requirement already satisfied: matplotlib in d:\\softwares\\anacondainstall\\lib\\site-packages (from gradio) (3.3.4)\n",
      "Requirement already satisfied: scipy in d:\\softwares\\anacondainstall\\lib\\site-packages (from gradio) (1.6.2)\n",
      "Requirement already satisfied: pandas in d:\\softwares\\anacondainstall\\lib\\site-packages (from gradio) (1.2.4)\n",
      "Requirement already satisfied: paramiko in d:\\softwares\\anacondainstall\\lib\\site-packages (from gradio) (2.7.2)\n",
      "Requirement already satisfied: numpy in d:\\softwares\\anacondainstall\\lib\\site-packages (from gradio) (1.20.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in d:\\softwares\\anacondainstall\\lib\\site-packages (from Flask>=1.1.1->gradio) (2.11.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in d:\\softwares\\anacondainstall\\lib\\site-packages (from Flask>=1.1.1->gradio) (1.0.1)\n",
      "Requirement already satisfied: click>=5.1 in d:\\softwares\\anacondainstall\\lib\\site-packages (from Flask>=1.1.1->gradio) (7.1.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in d:\\softwares\\anacondainstall\\lib\\site-packages (from Flask>=1.1.1->gradio) (1.1.0)\n",
      "Collecting Flask-Cors>=3.0.8\n",
      "  Using cached Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: Six in d:\\softwares\\anacondainstall\\lib\\site-packages (from Flask-Cors>=3.0.8->gradio) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\softwares\\anacondainstall\\lib\\site-packages (from Jinja2>=2.10.1->Flask>=1.1.1->gradio) (1.1.1)\n",
      "Collecting analytics-python\n",
      "  Downloading analytics_python-1.3.1-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: python-dateutil>2.1 in d:\\softwares\\anacondainstall\\lib\\site-packages (from analytics-python->gradio) (2.8.1)\n",
      "Collecting backoff==1.10.0\n",
      "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\softwares\\anacondainstall\\lib\\site-packages (from requests->gradio) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\softwares\\anacondainstall\\lib\\site-packages (from requests->gradio) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\softwares\\anacondainstall\\lib\\site-packages (from requests->gradio) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\softwares\\anacondainstall\\lib\\site-packages (from requests->gradio) (2020.12.5)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "Collecting flask-cachebuster\n",
      "  Using cached Flask_CacheBuster-1.0.0-py3-none-any.whl\n",
      "Collecting Flask-Login\n",
      "  Downloading Flask_Login-0.5.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting markdown2\n",
      "  Downloading markdown2-2.4.0-py2.py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\softwares\\anacondainstall\\lib\\site-packages (from matplotlib->gradio) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\softwares\\anacondainstall\\lib\\site-packages (from matplotlib->gradio) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in d:\\softwares\\anacondainstall\\lib\\site-packages (from matplotlib->gradio) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\softwares\\anacondainstall\\lib\\site-packages (from pandas->gradio) (2021.1)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in d:\\softwares\\anacondainstall\\lib\\site-packages (from paramiko->gradio) (3.2.0)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in d:\\softwares\\anacondainstall\\lib\\site-packages (from paramiko->gradio) (1.4.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in d:\\softwares\\anacondainstall\\lib\\site-packages (from paramiko->gradio) (3.4.7)\n",
      "Requirement already satisfied: cffi>=1.1 in d:\\softwares\\anacondainstall\\lib\\site-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.14.5)\n",
      "Requirement already satisfied: pycparser in d:\\softwares\\anacondainstall\\lib\\site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.20)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.10.1-cp35-abi3-win_amd64.whl (1.6 MB)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py): started\n",
      "  Building wheel for ffmpy (setup.py): finished with status 'done'\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4709 sha256=5a3bc8447e4f6bdb9180a534e341ed014c65cf759efb8b7ce7e51db12ce59991\n",
      "  Stored in directory: c:\\users\\jiss\\appdata\\local\\pip\\cache\\wheels\\ff\\5b\\59\\913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: monotonic, backoff, pycryptodome, markdown2, Flask-Login, Flask-Cors, flask-cachebuster, ffmpy, analytics-python, gradio\n",
      "Successfully installed Flask-Cors-3.0.10 Flask-Login-0.5.0 analytics-python-1.3.1 backoff-1.10.0 ffmpy-0.3.0 flask-cachebuster-1.0.0 gradio-2.0.2 markdown2-2.4.0 monotonic-1.6 pycryptodome-3.10.1\n"
     ]
    }
   ],
   "source": [
    "# ! pip install gradio"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputCols = [\"rotationRate\", \"userAcceleration\",\"weight\", \"height\", \"age\", \"gender\"]\n",
    "\n",
    "outputCol = \"features\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = spark.createDataFrame([(3.8651705114155055,1.6289638410434408,70.0,180.0,35.0,1.0)], inputCols)\n",
    "df_va = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "\n",
    "df = df_va.transform(df)\n",
    "\n",
    "prediction = LR_model.transform(df)\n",
    "prediction.select(\"prediction\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(prediction)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "result_pdf = prediction.select(\"*\").toPandas()\n",
    "result_pdf.prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selector(algo):\n",
    "    model = LR_model\n",
    "    if algo == \"Logistic regression\":\n",
    "        model = LR_model\n",
    "    elif algo == \"DT\":\n",
    "        model = DT_model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input(device):\n",
    "    acc1 = 0\n",
    "    gyro1 = 0\n",
    "    acc2 = 0\n",
    "    gyro2 = 0\n",
    "    \n",
    "    if device == \"Device 1\":\n",
    "        acc1 = 0.010253\n",
    "        gyro1 = 0.006959\n",
    "    elif device == \"Device 2\":\n",
    "        acc2 = 1.6289638410434408\n",
    "        gyro2 = 3.8651705114155055\n",
    "    else:\n",
    "        acc1 = 0.010253\n",
    "        gyro1 = 0.006959\n",
    "        acc2 = 0.010253\n",
    "        gyro2 = 0.006959\n",
    "        \n",
    "    return gyro1, acc1, gyro2, acc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(device, algo, gender, age, height, weight):\n",
    "    \n",
    "    if gender ==  \"Male\":\n",
    "        gender = 0\n",
    "    else:\n",
    "        gender = 1\n",
    "        \n",
    "    print(device, algo, gender, age, height, weight)\n",
    "    \n",
    "    prediction1 = None\n",
    "    prediction2 = None\n",
    "    \n",
    "    if device == \"Device 1\":\n",
    "        print(\"Device 1 Selected\")\n",
    "        gyro, acc, _ , _ = get_input(device)\n",
    "        df = spark.createDataFrame([(gyro,acc,weight, height, age, gender)], inputCols)\n",
    "        df_va = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "        df = df_va.transform(df)\n",
    "        model = model_selector(algo)\n",
    "        prediction = model.transform(df)\n",
    "        result_pdf = prediction.select(\"*\").toPandas()\n",
    "        prediction1 = result_pdf.prediction[0]\n",
    "        print(gyro, acc)\n",
    "        \n",
    "    elif device == \"Device 2\":\n",
    "        print(\"Device 2 Selected\")\n",
    "        _ , _ , gyro, acc = get_input(device)\n",
    "        df = spark.createDataFrame([(gyro,acc,weight, height, age, gender)], inputCols)\n",
    "        df_va = VectorAssembler(inputCols = inputCols, outputCol = outputCol)\n",
    "        df = df_va.transform(df)\n",
    "        model = model_selector(algo)\n",
    "        prediction = model.transform(df)\n",
    "        result_pdf = prediction.select(\"*\").toPandas()\n",
    "        prediction2 = result_pdf.prediction[0]\n",
    "        print(gyro, acc)\n",
    "        \n",
    "    else:\n",
    "        print(\"Device 1 & 2\")\n",
    "        \n",
    "        gryo1, acc1, gryo2, acc2 = get_input(device)\n",
    "        prediction1 = \"device 1\"\n",
    "        prediction2 = \"device 2\"\n",
    "        \n",
    "    \n",
    "\n",
    "    return prediction1, prediction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally at: http://127.0.0.1:7860/\n",
      "This share link will expire in 24 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted (NEW!)\n",
      "Running on External URL: https://16983.gradio.app\n",
      "Interface loading below...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://16983.gradio.app\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2f02184c070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Flask 'gradio.networking'>,\n",
       " 'http://127.0.0.1:7860/',\n",
       " 'https://16983.gradio.app')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 1 Logistic Regression 1 25 165 60\n",
      "Device 1 Selected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SparkMLBD\\Spark\\python\\pyspark\\sql\\pandas\\conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 1 Logistic Regression 1 25 165 60\n",
      "Device 1 Selected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-02 19:37:26,413] ERROR in app: Exception on /api/predict/ [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\flask_cors\\extension.py\", line 165, in wrapped_function\n",
      "    return cors_after_request(app.make_response(f(*args, **kwargs)))\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\gradio\\networking.py\", line 89, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\gradio\\networking.py\", line 171, in predict\n",
      "    prediction, durations = app.interface.process(raw_input)\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\gradio\\interface.py\", line 339, in process\n",
      "    predictions, durations = self.run_prediction(processed_input, return_duration=True)\n",
      "  File \"D:\\Softwares\\AnacondaInstall\\lib\\site-packages\\gradio\\interface.py\", line 312, in run_prediction\n",
      "    prediction = predict_fn(*processed_input)\n",
      "  File \"<ipython-input-53-d1e281af5fe8>\", line 21, in get_prediction\n",
      "    result_pdf = prediction.select(\"*\").toPandas()\n",
      "  File \"C:\\SparkMLBD\\Spark\\python\\pyspark\\sql\\pandas\\conversion.py\", line 141, in toPandas\n",
      "    pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)\n",
      "  File \"C:\\SparkMLBD\\Spark\\python\\pyspark\\sql\\dataframe.py\", line 677, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"C:\\SparkMLBD\\Spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"C:\\SparkMLBD\\Spark\\python\\pyspark\\sql\\utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"C:\\SparkMLBD\\Spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o549.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 265) (Predator executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n",
      "\tat java.lang.Thread.run(Unknown Source)\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n",
      "\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n",
      "\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n",
      "\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n",
      "\tat java.net.ServerSocket.accept(Unknown Source)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n",
      "\t... 29 more\r\n",
      "\n",
      "Driver stacktrace:\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n",
      "\tat scala.Option.foreach(Option.scala:407)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\r\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n",
      "\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\r\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n",
      "\tat java.lang.Thread.run(Unknown Source)\r\n",
      "Caused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n",
      "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n",
      "\t... 1 more\r\n",
      "Caused by: java.net.SocketTimeoutException: Accept timed out\r\n",
      "\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n",
      "\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n",
      "\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n",
      "\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n",
      "\tat java.net.ServerSocket.accept(Unknown Source)\r\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n",
      "\t... 29 more\r\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006959 0.010253\n",
      "Device 1 Logistic Regression 1 25 165 60\n",
      "Device 1 Selected\n",
      "0.006959 0.010253\n",
      "Device 1 Logistic Regression 1 25 165 60\n",
      "Device 1 Selected\n",
      "0.006959 0.010253\n",
      "Device 1 Logistic Regression 1 25 184 60\n",
      "Device 1 Selected\n",
      "0.006959 0.010253\n",
      "Device 1 Logistic Regression 1 25 184 64\n",
      "Device 1 Selected\n",
      "0.006959 0.010253\n",
      "Device 2 Logistic Regression 1 25 184 64\n",
      "Device 2 Selected\n",
      "3.8651705114155055 1.6289638410434408\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "img = gr.inputs.Image()\n",
    "\n",
    "title = \"Human Activity Recognition\"\n",
    "\n",
    "device = gr.inputs.Dropdown(choices = [\"Device 1\", \"Device 2\", \"Device 1 & 2\"], type=\"value\", label=\"Select Device\")\n",
    "\n",
    "algo = gr.inputs.Radio([\"Logistic Regression\", \"DT\"], label=\"Choose Algorithm\")\n",
    "\n",
    "gender = gr.inputs.CheckboxGroup(choices = [\"Male\", \"Female\"], type=\"value\", label=\"Gender\")\n",
    "age = gr.inputs.Slider(minimum = 20, maximum=80, step=1, default=25, label=\"Age\")\n",
    "height = gr.inputs.Slider(minimum = 100, maximum=200, step=1, default=180, label=\"Height\")\n",
    "weight = gr.inputs.Slider(minimum = 40, maximum=90, step=1, default=74, label=\"Weight\")\n",
    "        \n",
    "# prediction1 = gr.outputs.Dataframe(headers=None, type=\"auto\", label=None)\n",
    "prediction1 = gr.outputs.Textbox(type=\"auto\", label=\"Device 1\")\n",
    "prediction2 = gr.outputs.Textbox(type=\"auto\", label=\"Device 2\")\n",
    "\n",
    "gr.Interface(get_prediction, inputs = [device, algo, gender, age, height, weight],   \n",
    "             outputs = [prediction1, prediction2], \n",
    "             title=title,\n",
    "             live=False).launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
